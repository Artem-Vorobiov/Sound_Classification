{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2b3c5e",
   "metadata": {},
   "source": [
    "## <div align=\"center\">Data Preprocessing</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4483a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tables import *\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f151d667",
   "metadata": {},
   "source": [
    "### INTRODUCTION: in this Jupyter Notebook I will upload , preprocess and save data for SVM and K-Means. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3914a5",
   "metadata": {},
   "source": [
    "### Part 1. Start working with FILE data\n",
    "### Part 2. Prepare functions\n",
    "### Part 3. Run all the functions and prepare data for SVM and K-Means\n",
    "### Part 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef44c4f4",
   "metadata": {},
   "source": [
    "## <div align=\"center\">Part 1. Start working with FILE data</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9371e719",
   "metadata": {},
   "source": [
    "#### Read CSV. Chage .wav to .hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6db29326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/x7b5nvcx4bg27rbk_51dbkgm0000gn/T/ipykernel_40480/580465781.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['filename'] = df['filename'].str.replace('.wav', '.hdf5')\n"
     ]
    }
   ],
   "source": [
    "# specify YOUR PATH to the CSV file\n",
    "csv_path=os.path.join('ESC50_Home','ESC-50-HumanClassification.csv')\n",
    "\n",
    "# read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# check if all elements in the 'filename' column end with '.wav'\n",
    "result = (df['filename'].str.endswith('.wav')).all()\n",
    "\n",
    "# print the result\n",
    "print(result)\n",
    "\n",
    "# replace '.wav' with '.hdf5' in the 'filename' column\n",
    "df['filename'] = df['filename'].str.replace('.wav', '.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401c7726",
   "metadata": {},
   "source": [
    "#### Read categories from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40a40a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog' 'chirping_birds' 'vacuum_cleaner' 'thunderstorm' 'door_wood_knock'\n",
      " 'can_opening' 'crow' 'clapping' 'fireworks' 'chainsaw' 'airplane'\n",
      " 'mouse_click' 'pouring_water' 'train' 'sheep' 'water_drops'\n",
      " 'church_bells' 'clock_alarm' 'keyboard_typing' 'wind' 'footsteps' 'frog'\n",
      " 'cow' 'brushing_teeth' 'car_horn' 'crackling_fire' 'helicopter'\n",
      " 'drinking_sipping' 'rain' 'insects' 'laughing' 'hen' 'engine' 'breathing'\n",
      " 'crying_baby' 'hand_saw' 'coughing' 'glass_breaking' 'snoring'\n",
      " 'toilet_flush' 'pig' 'washing_machine' 'clock_tick' 'sneezing' 'rooster'\n",
      " 'sea_waves' 'siren' 'cat' 'door_wood_creaks' 'crickets']\n",
      "50\n",
      "\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "unique_categories = df['category'].unique()\n",
    "\n",
    "# print the unique categories\n",
    "print(unique_categories)\n",
    "\n",
    "count_of_unique_categories = df['category'].nunique()\n",
    "print(count_of_unique_categories)\n",
    "print(\"\")\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e27905f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['breathing', 'coughing', 'footsteps', 'laughing', 'sneezing', 'snoring', 'toilet_flush', 'vacuum_cleaner', 'washing_machine']\n",
      "\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# list of categories taken from a blackboard\n",
    "categories = [\n",
    "    'Breathing',\n",
    "    'Coughing',\n",
    "    'Footsteps',\n",
    "    'Laughing',\n",
    "    'Sneezing',\n",
    "    'Snoring',\n",
    "    'Toilet_flush',\n",
    "    'Vacuum_cleaner',\n",
    "    'Washing_machine',\n",
    "]\n",
    "\n",
    "# change the capital letter to lowercase\n",
    "categories = [category.lower() for category in categories]\n",
    "\n",
    "# print the modified list\n",
    "print(categories)\n",
    "print(\"\")\n",
    "print(\"Completed\")\n",
    "\n",
    "%timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedd9a32",
   "metadata": {},
   "source": [
    "#### Make a dictionary with keys=categories and values=category_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d748a163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of files 360.000000\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# Create empty dict for storing filenames for each category\n",
    "filenames_by_category  = {}\n",
    "number_of_files = 0\n",
    "\n",
    "# iterate over the categories\n",
    "for category in categories:\n",
    "    # get all the filenames for the current category\n",
    "    filenames = df.loc[df['category'].isin([category]), 'filename'].values.tolist()\n",
    "    number_of_files += len(filenames)\n",
    "    # create a key-value pair in the dictionary with the current category as the key and the list of filenames as the value\n",
    "    filenames_by_category[category] = filenames\n",
    "\n",
    "print(\"The number of files %f\" %number_of_files)\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e5c86",
   "metadata": {},
   "source": [
    "#### Make a dictionary with keys=categories and values=path_to_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7ab5195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "40\n",
      "The number of categories 9.000000\n",
      "The number of files 360.000000\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# Create empty dict for storing paths to files for each category\n",
    "hdf5_paths_per_category = {}\n",
    "\n",
    "# In order to control the flow of the code and not miss a change I keep trck on the number of files.\n",
    "counter_files = 0\n",
    "counter_categories = 0 \n",
    "filename = \"ptne\"\n",
    "\n",
    "# iterate over the categories\n",
    "for category in categories:\n",
    "    counter_categories += 1\n",
    "    \n",
    "    # Create an empty list where I store paths for one type of category\n",
    "    all_paths_for_same_category = list()\n",
    "    \n",
    "    # iterate over the list that belongs to exact category\n",
    "    for i in filenames_by_category[category]:\n",
    "        counter_files += 1\n",
    "        \n",
    "        # Create variable that store a single path\n",
    "        # CHANGE TO YOUR PATH\n",
    "        temp_hdf5_path=os.path.join('ESC50_Home', filename, i)\n",
    "        \n",
    "        # Store paths for one type of category in a list.\n",
    "        # With the new category the list will be updated to 0\n",
    "        all_paths_for_same_category.append(temp_hdf5_path)\n",
    "        \n",
    "    # Store a list according to the category\n",
    "    hdf5_paths_per_category[category] = all_paths_for_same_category\n",
    "\n",
    "\n",
    "print(type(hdf5_paths_per_category[\"snoring\"]))\n",
    "print(len(hdf5_paths_per_category[\"snoring\"]))\n",
    "\n",
    "print(\"The number of categories %f\" %counter_categories)\n",
    "print(\"The number of files %f\" %counter_files)\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ace54dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tract Labels: <KeysViewHDF5 ['energy', 'noise', 'pulse', 'tone']>\n",
      "Tract Labels: ['energy', 'noise', 'pulse', 'tone']\n",
      "Check one Label: noise\n"
     ]
    }
   ],
   "source": [
    "#hdf5 files read in using h5py modules\n",
    "tract_file = h5py.File(hdf5_paths_per_category[\"snoring\"][0], 'r')\n",
    "\n",
    "# Print out\n",
    "print(\"Tract Labels: %s\" % tract_file.keys())\n",
    "\n",
    "# Save Labels\n",
    "TRACT_Labels  = list(tract_file.keys()) # # Has 3 values inside\n",
    "\n",
    "# Keys are listed out\n",
    "print(\"Tract Labels: %s\" % TRACT_Labels)\n",
    "print(\"Check one Label: %s\" % TRACT_Labels[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a817f6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy\n",
      "noise\n",
      "pulse\n"
     ]
    }
   ],
   "source": [
    "tr_label_1 = TRACT_Labels[0]\n",
    "tr_label_2 = TRACT_Labels[1]\n",
    "tr_label_3 = TRACT_Labels[2]\n",
    "\n",
    "print(tr_label_1)\n",
    "print(tr_label_2)\n",
    "print(tr_label_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e886894f",
   "metadata": {},
   "source": [
    "## <div align=\"center\">Part 2. Prepare functions</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23bb0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_arr_all_files_of_category(category_name, labels):\n",
    "    \"\"\" This code extracts data from hdf5 files, access the s_tract dataset in each file,  \n",
    "    and append a portion of that data to a list called list_of_data\"\"\"\n",
    "    \n",
    "    counter_files = 0\n",
    "    list_of_data = list() # Create list. Here we will store all arrays for 1 category\n",
    "    \n",
    "    for i in hdf5_paths_per_category[category_name]:\n",
    "        \n",
    "        # Here we iterate over every file of one category\n",
    "        with h5py.File(i, 'r') as f:\n",
    "#             print(f)\n",
    "            \n",
    "            # access the \"s_tract\" dataset in the file\n",
    "            s_tract = f[labels][()]\n",
    "            \n",
    "            # Append a file and get rid of zeros in the bottom and in the top\n",
    "            list_of_data.append(s_tract)\n",
    "        \n",
    "        counter_files += 1\n",
    "        \n",
    "#     print(\"Completed\")\n",
    "    return list_of_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8957c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_arrat_func(file_2D_data, sep_number):\n",
    "    \n",
    "    # Preparation before splitting\n",
    "    length_of_data = len(file_2D_data)\n",
    "    \n",
    "    # Split without remain\n",
    "    sub_arr_size = length_of_data // sep_number\n",
    "    safe_separate = sub_arr_size * sep_number\n",
    "    \n",
    "    # The main part - splitting\n",
    "    splited_lists = np.split(np.array(file_2D_data[:safe_separate]), sep_number)\n",
    "\n",
    "    return splited_lists\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228cd68d",
   "metadata": {},
   "source": [
    "Here I get my features.\n",
    "I am going to give bins=4 and thus, have 4 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1733cc",
   "metadata": {},
   "source": [
    " Based on observation of a histograms in file 1.0, I decided do not take values that are over 100.\n",
    " Thus I can keep temporal alignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d96eb903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(splited_array):\n",
    "    list_of_features = np.array([]) # 1\n",
    "    # Features consistency cut out the border.\n",
    "    \n",
    "    for i in splited_array:\n",
    "        \n",
    "#         Every array has a shape  (14, 8498) or (14, 8497). It does not matter for us because we will flatten them.\n",
    "        arr_one_flat = i.flatten() # 246413\n",
    "        \n",
    "        # Based on observation of a histogram, I decided do not take values that are over 175\n",
    "        # Thus I can keep temporal alignment. \n",
    "        arr_one_flat = arr_one_flat[arr_one_flat <= 100]\n",
    "\n",
    "        hist1, bins1 = np.histogram(arr_one_flat,bins=4)\n",
    "        list_of_features = np.append(list_of_features, hist1) \n",
    "\n",
    "    \n",
    "    return list_of_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae959b",
   "metadata": {},
   "source": [
    "## The core function where we link all the function above and make labels and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03bcd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_SVM(array_with_catigories):\n",
    "\n",
    "    final_label = list()\n",
    "    final_features = list()\n",
    "    \n",
    "    # Iterate over labelf of HDF5 file - ['E', 'f_tract', 's_tract']\n",
    "    for label in TRACT_Labels:\n",
    "    \n",
    "        all_categories_features = list()  # Global array for features\n",
    "        label_array = list()              # For collecting labels\n",
    "        num_label = 0       # Every iteration of a category num_label will +1 and label_array will recieve new values\n",
    "\n",
    "        for i in array_with_catigories:\n",
    "            print(\"\")\n",
    "            print(\"Category : \", i)\n",
    "            one_category_features = list() # Local array, for features only for one category \n",
    "\n",
    "            arr_all_files_one_category = create_arr_all_files_of_category(i, label) # List, all of 40 files of ine category.\n",
    "\n",
    "            # Get access to individual file\n",
    "            for i in arr_all_files_one_category:\n",
    "\n",
    "                splited_lists = split_arrat_func(i, 1)  # list (6). \n",
    "                hey = get_feature(splited_lists) #  np.array - get features for one file - for example (60,) if I have 6 subarray and 10 bins\n",
    "                one_category_features.append(hey) # list - store features in the list\n",
    "\n",
    "            one_category_features = np.array(one_category_features) # (40, 60) - feature set(60) for 40 samples - one category\n",
    "            all_categories_features.append(one_category_features) # store seatures set in a list. Len will be 9.\n",
    "\n",
    "            label_1 = np.zeros(len(arr_all_files_one_category), dtype=int) + num_label\n",
    "\n",
    "            label_array.append(label_1) \n",
    "\n",
    "            num_label += 1\n",
    "\n",
    "\n",
    "        print(\"Saving...\")\n",
    "\n",
    "        # We cannot accept this shape because SVM get recieve only 2D array for features and 1D for labels\n",
    "        all_categories_features = np.array(all_categories_features) # (9, 40, 60) - 9 categories, 40 samples, 60 features\n",
    "\n",
    "        label_array = np.array(label_array) # Shape (9, 40)\n",
    "\n",
    "        # This part is rehsaping \n",
    "        num_of_samples = len(arr_all_files_one_category) # = 40\n",
    "        num_of_categories = len(array_with_catigories) # = 9\n",
    "        num_rows = num_of_samples*num_of_categories # = 360\n",
    "\n",
    "        num_of_col = len(hey) # = num of features for 1 file\n",
    "\n",
    "        all_categories_features = all_categories_features.reshape(num_rows,num_of_col) # Shape (360, 60) for all 9 categories\n",
    "        label_array = label_array.reshape(num_rows) # Shape (360,) for all 9 categories\n",
    "\n",
    "    \n",
    "        final_features.append(all_categories_features)\n",
    "        final_label.append(label_array)\n",
    "        \n",
    "    final_features = np.array(final_features)\n",
    "    final_label = np.array(final_label)\n",
    "    \n",
    "    # Prepre variablef for reshaping of finale features\n",
    "    num_rows_2 = int(len(final_features) * len(final_features[0]))\n",
    "    num_of_col_2 = int(len(final_features[0][0]))\n",
    "\n",
    "\n",
    "    print(final_features.shape)\n",
    "    print(type(num_rows_2))\n",
    "    print(num_rows_2)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Reshaping\n",
    "    final_features_done = final_features.reshape(num_rows_2, num_of_col_2) # Shape (360, 60) for all 9 categories\n",
    "    final_label_done = final_label.reshape(num_rows_2) # Shape (360,) for all 9 categories\n",
    "    \n",
    "    # SHAPE CONTROL\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Completed prepare_data_for_SVM\")\n",
    "    \n",
    "    return final_features_done, final_label_done, final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82cb16d",
   "metadata": {},
   "source": [
    "## <div align=\"center\">Part 3. Run all the functions and prepare data for SVM  and K-Means</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38b8d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_category = ['breathing', 'footsteps']\n",
    "# features_array,label_array, test_hist = prepare_data_for_SVM(test_category) # \"QUICKTEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92437d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category :  breathing\n",
      "\n",
      "Category :  coughing\n",
      "\n",
      "Category :  footsteps\n",
      "\n",
      "Category :  laughing\n",
      "\n",
      "Category :  sneezing\n",
      "\n",
      "Category :  snoring\n",
      "\n",
      "Category :  toilet_flush\n",
      "\n",
      "Category :  vacuum_cleaner\n",
      "\n",
      "Category :  washing_machine\n",
      "Saving...\n",
      "\n",
      "Category :  breathing\n",
      "\n",
      "Category :  coughing\n",
      "\n",
      "Category :  footsteps\n",
      "\n",
      "Category :  laughing\n",
      "\n",
      "Category :  sneezing\n",
      "\n",
      "Category :  snoring\n",
      "\n",
      "Category :  toilet_flush\n",
      "\n",
      "Category :  vacuum_cleaner\n",
      "\n",
      "Category :  washing_machine\n",
      "Saving...\n",
      "\n",
      "Category :  breathing\n",
      "\n",
      "Category :  coughing\n",
      "\n",
      "Category :  footsteps\n",
      "\n",
      "Category :  laughing\n",
      "\n",
      "Category :  sneezing\n",
      "\n",
      "Category :  snoring\n",
      "\n",
      "Category :  toilet_flush\n",
      "\n",
      "Category :  vacuum_cleaner\n",
      "\n",
      "Category :  washing_machine\n",
      "Saving...\n",
      "\n",
      "Category :  breathing\n",
      "\n",
      "Category :  coughing\n",
      "\n",
      "Category :  footsteps\n",
      "\n",
      "Category :  laughing\n",
      "\n",
      "Category :  sneezing\n",
      "\n",
      "Category :  snoring\n",
      "\n",
      "Category :  toilet_flush\n",
      "\n",
      "Category :  vacuum_cleaner\n",
      "\n",
      "Category :  washing_machine\n",
      "Saving...\n",
      "(4, 360, 4)\n",
      "<class 'int'>\n",
      "1440\n",
      "\n",
      "\n",
      "Completed prepare_data_for_SVM\n"
     ]
    }
   ],
   "source": [
    "features_array,label_array, test_hist = prepare_data_for_SVM(categories) # \"FULLTEST\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a1066",
   "metadata": {},
   "source": [
    "Save the data for easy access in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43324c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving\n",
    "np.save('features_array_ptne_1.0.npy', features_array)\n",
    "np.save('label_array_ptne_1.0.npy', label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eab954ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1440, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(1440,)\n",
      "Cool\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes if they're ready to feed SVM\n",
    "print(type(features_array))\n",
    "print(features_array.shape)\n",
    "\n",
    "print(type(label_array))\n",
    "print(label_array.shape)\n",
    "\n",
    "print(\"Cool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a6409",
   "metadata": {},
   "source": [
    "## <div align=\"center\">Part 5. Conclusion</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967c869",
   "metadata": {},
   "source": [
    "1. Temporal alignment is applied \n",
    "2. Multiple feature sets are used \n",
    "3. Data is prepared for the SVM and K-Means"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
